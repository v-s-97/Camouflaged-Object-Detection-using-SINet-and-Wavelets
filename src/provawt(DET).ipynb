{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846d28c2",
   "metadata": {},
   "source": [
    "# SINet COD10K Detection\n",
    "Questo notebook implementa SINet per il rilevamento di oggetti mimetizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqq pytorch_wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba989901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pywt\n",
    "from pytorch_wavelets import DWTForward\n",
    "import time\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f24b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "# tensor = torch.randn((2,3), device = torch.device('cuda:0'))\n",
    "# print(tensor.device)\n",
    "# device = torch.device('cuda:0')\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# device = torch.device('mps')\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \\\n",
    "         torch.device(\"cuda\") if torch.cuda.is_available() else \\\n",
    "         torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ea9e7",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a294171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CODDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        super(CODDataset, self).__init__()\n",
    "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.img_paths) == len(self.mask_paths), \\\n",
    "            \"Numero di immagini e maschere non coincide!\"\n",
    "\n",
    "        # Definiamo una trasformazione di base per il ridimensionamento\n",
    "        self.resize = transforms.Resize((224, 224)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Converte la maschera in scala di grigi\n",
    "\n",
    "        # Applichiamo il ridimensionamento\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        # Se ci sono altre trasformazioni, le applichiamo\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "\n",
    "        # Convertiamo la maschera in binaria\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0144d4f",
   "metadata": {},
   "source": [
    "### Backbone feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df8c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        resnet = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "\n",
    "        self.stage1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n",
    "        self.pool = resnet.maxpool\n",
    "        self.stage2 = resnet.layer1\n",
    "        self.stage3 = resnet.layer2\n",
    "        self.stage4 = resnet.layer3\n",
    "        self.stage5 = resnet.layer4\n",
    "\n",
    "        # Inizializzazione del DWT con wavelet Harr ('db2')\n",
    "        self.dwt = DWTForward(J=1, mode='zero', wave='db2').to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.stage1(x)\n",
    "        x1p = self.pool(x1)\n",
    "        x2 = self.stage2(x1p)\n",
    "        x3 = self.stage3(x2)\n",
    "        x4 = self.stage4(x3)\n",
    "        x5 = self.stage5(x4)\n",
    "\n",
    "        def apply_wavelet(feature_map, original_size):\n",
    "            \"\"\"Applica la trasformata wavelet e riporta la dimensione spaziale all'originale.\"\"\"\n",
    "            feature_map = feature_map.to(device)  # Assicura che sia sul giusto device\n",
    "            Yl, Yh = self.dwt(feature_map)\n",
    "\n",
    "            # Separiamo i coefficienti LH, HL, HH\n",
    "            cH, cV, cD = torch.chunk(Yh[0], chunks=3, dim=2)  # LH, HL, HH\n",
    "\n",
    "            # Assicuriamoci che Yl abbia la stessa dimensione di output\n",
    "            Yl = F.interpolate(Yl, size=original_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Upsample per mantenere la stessa dimensione dell'input originale\n",
    "            cH = F.interpolate(cH.squeeze(2), size=original_size, mode='bilinear', align_corners=False)\n",
    "            cV = F.interpolate(cV.squeeze(2), size=original_size, mode='bilinear', align_corners=False)\n",
    "            cD = F.interpolate(cD.squeeze(2), size=original_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Concatenazione di tutte le componenti wavelet\n",
    "            wavelet_features = torch.cat([Yl, cH, cV, cD], dim=1)\n",
    "            return wavelet_features\n",
    "\n",
    "        # Applicazione della wavelet con ridimensionamento\n",
    "        x2_freq = apply_wavelet(x2, x2.shape[2:])\n",
    "        x3_freq = apply_wavelet(x3, x3.shape[2:])\n",
    "        x4_freq = apply_wavelet(x4, x4.shape[2:])\n",
    "        x5_freq = apply_wavelet(x5, x5.shape[2:])\n",
    "\n",
    "        return x1, x2, x3, x4, x5, x2_freq, x3_freq, x4_freq, x5_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e050d",
   "metadata": {},
   "source": [
    "### Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08adbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchModule(nn.Module):\n",
    "    def __init__(self, in_channels_list=[256, 512, 1024]):\n",
    "        super(SearchModule, self).__init__()\n",
    "\n",
    "        self.freq_compression = nn.ModuleList([\n",
    "            nn.Conv2d(4 * in_ch, in_ch, kernel_size=1)  # Invece di ridurre a 4 canali, manteniamo più informazioni\n",
    "            for in_ch in in_channels_list\n",
    "        ])\n",
    "\n",
    "        self.conv_list = nn.ModuleList([\n",
    "            nn.Conv2d(2 * in_ch, 256, kernel_size=3, padding=1)  # Integriamo meglio le feature wavelet\n",
    "            for in_ch in in_channels_list\n",
    "        ])\n",
    "\n",
    "        self.out_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x2, x3, x4, x2_freq, x3_freq, x4_freq):\n",
    "        x2_freq = self.freq_compression[0](x2_freq)\n",
    "        x3_freq = self.freq_compression[1](x3_freq)\n",
    "        x4_freq = self.freq_compression[2](x4_freq)\n",
    "\n",
    "        x2_ = self.conv_list[0](torch.cat([x2, x2_freq], dim=1))\n",
    "        x3_ = F.interpolate(self.conv_list[1](torch.cat([x3, x3_freq], dim=1)),\n",
    "                            size=x2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x4_ = F.interpolate(self.conv_list[2](torch.cat([x4, x4_freq], dim=1)),\n",
    "                            size=x2.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        fused = x2_ + x3_ + x4_\n",
    "        coarse_map = self.out_conv(fused)\n",
    "        coarse_map = torch.sigmoid(coarse_map)\n",
    "\n",
    "        return coarse_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67098bbc",
   "metadata": {},
   "source": [
    "### Identification Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2387e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationModule(nn.Module):\n",
    "    def __init__(self, in_channels=2048):\n",
    "        super(IdentificationModule, self).__init__()\n",
    "\n",
    "        self.freq_compression = nn.Conv2d(4 * in_channels, 4, kernel_size=1)\n",
    "        self.conv_deep = nn.Conv2d(in_channels + 4, 256, kernel_size=3, padding=1)\n",
    "        self.refine_conv = nn.Conv2d(256 + 256, 256, kernel_size=3, padding=1)\n",
    "        self.out_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "        self.coarse_map_expand = nn.Conv2d(1, 256, kernel_size=1)\n",
    "\n",
    "    def forward(self, x5, x5_freq, coarse_map):\n",
    "        x5_freq = self.freq_compression(x5_freq)\n",
    "\n",
    "        coarse_map = F.interpolate(coarse_map, size=x5.shape[2:], mode='bilinear', align_corners=False)\n",
    "        coarse_map = self.coarse_map_expand(coarse_map)\n",
    "        x5_ = self.conv_deep(torch.cat([x5, x5_freq], dim=1))\n",
    "        x5_up = F.interpolate(x5_, scale_factor=8, mode='bilinear', align_corners=False)\n",
    "\n",
    "        coarse_map = F.interpolate(coarse_map, size=x5_up.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        refine_input = torch.cat([x5_up, coarse_map], dim=1)\n",
    "        refine_feat = self.refine_conv(refine_input)\n",
    "        out_map = self.out_conv(refine_feat)\n",
    "        out_map = torch.sigmoid(out_map)\n",
    "\n",
    "        return out_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3992f9",
   "metadata": {},
   "source": [
    "### SINet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6483a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SINet(nn.Module):\n",
    "    def __init__(self, backbone_pretrained=True):\n",
    "        super(SINet, self).__init__()\n",
    "        self.backbone = ResNetBackbone(pretrained=backbone_pretrained)\n",
    "        self.search = SearchModule(in_channels_list=[256, 512, 1024])\n",
    "        self.identify = IdentificationModule(in_channels=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5, x2_freq, x3_freq, x4_freq, x5_freq = self.backbone(x)\n",
    "        coarse_map = self.search(x2, x3, x4, x2_freq, x3_freq, x4_freq)\n",
    "        refine_map = self.identify(x5, x5_freq, coarse_map)\n",
    "        out_final = F.interpolate(refine_map, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return out_final, coarse_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33f5ca",
   "metadata": {},
   "source": [
    "### Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343bec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_metrics(pred, target, threshold=0.5):\n",
    "\n",
    "    pred_bin = (pred >= threshold).float()\n",
    "\n",
    "    eps = 1e-7\n",
    "    batch_size = pred.shape[0]\n",
    "\n",
    "    acc_list, prec_list, rec_list, f1_list, iou_list = [], [], [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        p = pred_bin[i].view(-1)   \n",
    "        t = target[i].view(-1)    \n",
    "\n",
    "        TP = (p * t).sum().item()\n",
    "        FP = (p * (1 - t)).sum().item()\n",
    "        FN = ((1 - p) * t).sum().item()\n",
    "        TN = ((1 - p) * (1 - t)).sum().item()\n",
    "\n",
    "\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN + eps)\n",
    "       \n",
    "        prec = TP / (TP + FP + eps)\n",
    "\n",
    "        rec = TP / (TP + FN + eps)\n",
    "\n",
    "        f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "  \n",
    "        union = TP + FP + FN\n",
    "        iou = TP / (union + eps)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        prec_list.append(prec)\n",
    "        rec_list.append(rec)\n",
    "        f1_list.append(f1)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': np.mean(acc_list),\n",
    "        'precision': np.mean(prec_list),\n",
    "        'recall': np.mean(rec_list),\n",
    "        'f1': np.mean(f1_list),\n",
    "        'iou': np.mean(iou_list)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def S_measure(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola la similarità strutturale tra la mappa predetta e la ground truth.\n",
    "    \"\"\"\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    gt_np = gt.squeeze().cpu().numpy()\n",
    "    return ssim(pred_np, gt_np, data_range=1.0)\n",
    "\n",
    "def E_measure(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola la E-measure combinando precisione locale e statistiche globali.\n",
    "    \"\"\"\n",
    "    pred = pred.squeeze().cpu().numpy()\n",
    "    gt = gt.squeeze().cpu().numpy()\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    \n",
    "    # Similarità tra pixel con pesi\n",
    "    diff = np.abs(pred - gt)\n",
    "    return 1 - np.mean(diff)\n",
    "\n",
    "def weighted_F_measure(pred, gt, beta=1):\n",
    "    \"\"\"\n",
    "    Calcola il weighted F-measure per valutare la qualità della segmentazione.\n",
    "    \"\"\"\n",
    "    pred = pred.squeeze().cpu().numpy() > 0.5\n",
    "    gt = gt.squeeze().cpu().numpy() > 0.5\n",
    "    TP = np.sum(pred * gt)\n",
    "    FP = np.sum(pred * (1 - gt))\n",
    "    FN = np.sum((1 - pred) * gt)\n",
    "    \n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    \n",
    "    F_beta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + 1e-8)\n",
    "    return F_beta\n",
    "\n",
    "def mean_absolute_error(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola l'errore medio assoluto tra la mappa predetta e la ground truth.\n",
    "    \"\"\"\n",
    "    return torch.abs(pred - gt).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f3cde",
   "metadata": {},
   "source": [
    "### Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e93c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.0):\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n",
    "\n",
    "def boundary_loss(pred, target):\n",
    "    kernel = torch.tensor([[[[-1, 0, 1], \n",
    "                             [-2, 0, 2], \n",
    "                             [-1, 0, 1]]]], dtype=torch.float32, device=target.device)  # Forziamo float32\n",
    "    \n",
    "    edge_target = torch.abs(F.conv2d(target, kernel, padding=1))\n",
    "    edge_pred = torch.abs(F.conv2d(pred, kernel, padding=1))\n",
    "\n",
    "    return F.l1_loss(edge_pred, edge_target)\n",
    "\n",
    "\n",
    "def combined_loss(pred, target):\n",
    "    return dice_loss(pred, target) + F.binary_cross_entropy(pred, target) + boundary_loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678f397",
   "metadata": {},
   "source": [
    "### Train Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f611c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        #print('prova', images.device, masks.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out_final, out_coarse = model(images)\n",
    "\n",
    "        loss_final = dice_loss(out_final, masks) + F.binary_cross_entropy(out_final, masks)\n",
    "\n",
    "        loss_coarse = dice_loss(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest')) \\\n",
    "                      + F.binary_cross_entropy(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest'))\n",
    "\n",
    "        loss = loss_final + 0.5 * loss_coarse\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_acc, all_prec, all_rec, all_f1, all_iou = [], [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            out_final, out_coarse = model(images)\n",
    "\n",
    "            loss_final = dice_loss(out_final, masks) + F.binary_cross_entropy(out_final, masks)\n",
    "            loss_coarse = dice_loss(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest')) \\\n",
    "                          + F.binary_cross_entropy(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest'))\n",
    "            loss = loss_final + 0.5 * loss_coarse\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            batch_metrics = compute_batch_metrics(out_final, masks, threshold=0.5)\n",
    "            all_acc.append(batch_metrics['accuracy'])\n",
    "            all_prec.append(batch_metrics['precision'])\n",
    "            all_rec.append(batch_metrics['recall'])\n",
    "            all_f1.append(batch_metrics['f1'])\n",
    "            all_iou.append(batch_metrics['iou'])\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(all_acc),\n",
    "        'precision': np.mean(all_prec),\n",
    "        'recall': np.mean(all_rec),\n",
    "        'f1': np.mean(all_f1),\n",
    "        'iou': np.mean(all_iou)\n",
    "    }\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ea930",
   "metadata": {},
   "source": [
    "### Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8944f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(mask_np, pred_np):\n",
    "\n",
    "    mask_np = mask_np.flatten()\n",
    "    pred_np = pred_np.flatten()\n",
    "    if mask_np.sum() == 0:\n",
    "        return None\n",
    "    return jaccard_score(mask_np, pred_np, zero_division=1)\n",
    "\n",
    "def evaluate_segmentation(mask_np, pred_np):\n",
    "    mask_np = mask_np.flatten()\n",
    "    pred_np = pred_np.flatten()\n",
    "\n",
    "    iou = jaccard_score(mask_np, pred_np, average=\"binary\", zero_division=1)\n",
    "    dice = f1_score(mask_np, pred_np, average=\"binary\")\n",
    "    precision = precision_score(mask_np, pred_np, average=\"binary\")\n",
    "    recall = recall_score(mask_np, pred_np, average=\"binary\")\n",
    "\n",
    "    return {\n",
    "        \"IoU\": iou,\n",
    "        \"Dice\": dice,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall\n",
    "    }\n",
    "\n",
    "def test_model(model, dataloader, device, threshold=0.5, num_samples=20):\n",
    "    model.eval()\n",
    "    all_acc, all_prec, all_rec, all_f1, all_iou = [], [], [], [], []\n",
    "    all_prec2, all_rec2, all_dice, all_iou2 = [], [], [], []  # forse da eliminare se non utile alla valutazione\n",
    "    all_s_measure, all_e_measure, all_weighted_f_measure, all_mean_absolute_error = [], [], [], []  # forse da eliminare se non utile alla valutazione\n",
    "\n",
    "    iou_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(len(dataloader))\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            print(f\"Processing batch {i+1}/{len(dataloader)}...\")  # DEBUG\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            out_final, out_coarse = model(images)\n",
    "            print(\"Model inference done.\")  # DEBUG\n",
    "\n",
    "            batch_metrics = compute_batch_metrics(out_final, masks, threshold=threshold)\n",
    "            print(\"Metrics computed.\")  # DEBUG\n",
    "\n",
    "            s_measure = S_measure(out_final, masks)\n",
    "            e_measure = E_measure(out_final, masks)\n",
    "            weighted_f_measure = weighted_F_measure(out_final,masks)\n",
    "            mean_Absolute_Error = mean_absolute_error(out_final, masks)\n",
    "\n",
    "            all_s_measure.append(s_measure)     # forse da eliminare se non utile alla valutazione\n",
    "            all_e_measure.append(e_measure)\n",
    "            all_weighted_f_measure.append(weighted_f_measure)\n",
    "            all_mean_absolute_error.append(mean_Absolute_Error)\n",
    "\n",
    "            mask_np = (masks.squeeze().cpu().numpy() > 0.5).astype(int)\n",
    "            pred_np = (out_final.squeeze().cpu().numpy() > 0.5).astype(int)\n",
    "            iou = compute_iou(mask_np, pred_np)\n",
    "\n",
    "            second_batch_metrics = evaluate_segmentation(mask_np, pred_np)   # forse da eliminare se non utile alla valutazione\n",
    "\n",
    "            all_prec2.append(second_batch_metrics['Precision'])     # forse da eliminare se non utile alla valutazione\n",
    "            all_rec2.append(second_batch_metrics['Recall'])\n",
    "            all_dice.append(second_batch_metrics['Dice'])\n",
    "            all_iou2.append(second_batch_metrics['IoU'])\n",
    "\n",
    "            if iou is not None:\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "            all_acc.append(batch_metrics['accuracy'])\n",
    "            all_prec.append(batch_metrics['precision'])\n",
    "            all_rec.append(batch_metrics['recall'])\n",
    "            all_f1.append(batch_metrics['f1'])\n",
    "            all_iou.append(batch_metrics['iou'])\n",
    "\n",
    "            # Plot delle prime immagini\n",
    "            if i < num_samples:\n",
    "                plot_results(images.cpu(), masks.cpu(), out_final.cpu(), i)\n",
    "\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(all_acc),\n",
    "        'precision': np.mean(all_prec),\n",
    "        'recall': np.mean(all_rec),\n",
    "        'f1': np.mean(all_f1),\n",
    "        'iou': np.mean(all_iou)\n",
    "    }\n",
    "\n",
    "    avg_metrics2 = {                             # forse da eliminare se non utile alla valutazione\n",
    "        'Precision': np.mean(all_prec2),\n",
    "        'Recall': np.mean(all_rec2),\n",
    "        'Dice': np.mean(all_dice),\n",
    "        'IoU': np.mean(all_iou2)\n",
    "    }\n",
    "\n",
    "    avg_metrics3 = {                             # forse da eliminare se non utile alla valutazione\n",
    "        'S-Measure': np.mean(all_s_measure),\n",
    "        'E-Measure': np.mean(all_e_measure),\n",
    "        'WFM': np.mean(all_weighted_f_measure),\n",
    "        'MAE': np.mean(all_mean_absolute_error)\n",
    "    }\n",
    "\n",
    "    print(f\"Mean IoU (filtered): {sum(iou_scores) / len(iou_scores):.2f}\")\n",
    "    return avg_metrics, avg_metrics2, avg_metrics3\n",
    "\n",
    "def plot_results(images, masks, predictions, batch_idx):\n",
    "    \"\"\"\n",
    "    Mostra le immagini originali, le maschere reali e le previsioni del modello.\n",
    "    \"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    fig, axes = plt.subplots(batch_size, 3, figsize=(10, batch_size * 3))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = images[i].permute(1, 2, 0).numpy()  # Converti da tensor a numpy\n",
    "        mask = masks[i].squeeze().numpy()\n",
    "        pred = (predictions[i].squeeze().numpy() > 0.5).astype(np.uint8)  # Applica threshold\n",
    "\n",
    "        if batch_size == 1:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(\"Immagine originale\")\n",
    "            axes[1].imshow(mask, cmap=\"gray\")\n",
    "            axes[1].set_title(\"Maschera reale\")\n",
    "            axes[2].imshow(pred, cmap=\"gray\")\n",
    "            axes[2].set_title(\"Previsione modello\")\n",
    "        else:\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f\"Immagine {batch_idx * batch_size + i}\")\n",
    "            axes[i, 1].imshow(mask, cmap=\"gray\")\n",
    "            axes[i, 1].set_title(\"Maschera reale\")\n",
    "            axes[i, 2].imshow(pred, cmap=\"gray\")\n",
    "            axes[i, 2].set_title(\"Previsione modello\")\n",
    "\n",
    "        for ax in axes[i] if batch_size > 1 else axes:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499aeafd",
   "metadata": {},
   "source": [
    "### Train split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793c80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dataset(train_dir, val_ratio=0.2):\n",
    "    train_img_dir = os.path.join(train_dir, 'Image')\n",
    "    train_mask_dir = os.path.join(train_dir, 'GT_Object')\n",
    "    val_img_dir = os.path.join(train_dir, '../Validation/Image')\n",
    "    val_mask_dir = os.path.join(train_dir, '../Validation/GT_Object')\n",
    "\n",
    "    os.makedirs(val_img_dir, exist_ok=True)\n",
    "    os.makedirs(val_mask_dir, exist_ok=True)\n",
    "\n",
    "    image_files = sorted(glob.glob(os.path.join(train_img_dir, '*.jpg')))\n",
    "    mask_files = sorted(glob.glob(os.path.join(train_mask_dir, '*.png')))\n",
    "\n",
    "    assert len(image_files) == len(mask_files), \"Il numero di immagini e maschere non coincide!\"\n",
    "\n",
    "    random.seed(42)\n",
    "    indices = list(range(len(image_files)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(len(indices) * (1 - val_ratio))\n",
    "    train_indices, val_indices = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "    for idx in val_indices:\n",
    "        shutil.move(image_files[idx], os.path.join(val_img_dir, os.path.basename(image_files[idx])))\n",
    "        shutil.move(mask_files[idx], os.path.join(val_mask_dir, os.path.basename(mask_files[idx])))\n",
    "\n",
    "    print(f\"Train: {len(train_indices)}, Validation: {len(val_indices)}\")\n",
    "    return train_img_dir, train_mask_dir, val_img_dir, val_mask_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857df341",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di utilizzo\n",
    "train_dataset_dir = 'COD10K-v3/Train'  # Sostituire con il percorso reale\n",
    "\n",
    "\n",
    "def main():\n",
    "    batch_size = 16\n",
    "    num_epochs = 30\n",
    "    lr = 1e-4\n",
    "\n",
    "\n",
    "    # train_img_dir, train_mask_dir, val_img_dir, val_mask_dir = split_train_dataset(train_dataset_dir)\n",
    "    train_img_dir = 'COD10K-v3/Train/Image'\n",
    "    train_mask_dir = 'COD10K-v3/Train/GT_Object'\n",
    "    val_img_dir = 'COD10K-v3/Validation/Image'\n",
    "    val_mask_dir = 'COD10K-v3/Validation/GT_Object'\n",
    "    \n",
    "    \n",
    "    test_img_dir = 'COD10K-v3/Test/Image'\n",
    "    test_mask_dir = 'COD10K-v3/Test/GT_Object'\n",
    "\n",
    "    os.makedirs(val_img_dir, exist_ok=True)\n",
    "    os.makedirs(val_mask_dir, exist_ok=True)\n",
    "\n",
    "# Ottieni tutti i file\n",
    "    image_files = sorted(glob.glob(os.path.join(test_img_dir, '*.jpg')))\n",
    "    mask_files = sorted(glob.glob(os.path.join(test_mask_dir, '*.png')))\n",
    "\n",
    "# Assicurati che il numero di immagini e maschere corrisponda\n",
    "    assert len(image_files) == len(mask_files), \"Numero di immagini e maschere non coincide!\"\n",
    "\n",
    "# Mescola gli indici\n",
    "    random.seed(42)  # Per la riproducibilità\n",
    "    indices = list(range(len(image_files)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "# Definisci il rapporto di split (es. 80% test, 20% validation)\n",
    "    split_idx = int(len(indices) * 0.8)\n",
    "    test_indices, val_indices = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# Sposta i file nelle rispettive cartelle\n",
    "    for idx in val_indices:\n",
    "        shutil.move(image_files[idx], os.path.join(val_img_dir, os.path.basename(image_files[idx])))\n",
    "        shutil.move(mask_files[idx], os.path.join(val_mask_dir, os.path.basename(mask_files[idx])))\n",
    "\n",
    "    print(f\"Train images: {len(os.listdir(train_img_dir))}\")\n",
    "    print(f\"Train masks: {len(os.listdir(train_mask_dir))}\")\n",
    "\n",
    "    train_dataset = CODDataset(train_img_dir, train_mask_dir, transform=None)\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample keys: {sample.keys()}\" if isinstance(sample, dict) else \"Sample loaded\")\n",
    "    val_dataset = CODDataset(val_img_dir, val_mask_dir, transform=None)\n",
    "    test_dataset = CODDataset(test_img_dir, test_mask_dir, transform=None)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = SINet(backbone_pretrained=True).to(device)\n",
    "    #print(next(model.parameters()).device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_metrics = validate_one_epoch(model, val_loader, device)  # Scompatta la tupla\n",
    "        epoch_duration = timedelta(seconds=time.time() - start_time)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch Duration: {epoch_duration}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"sinet_camouflage.pth\")\n",
    "    print(\"Training completato e modello salvato.\")\n",
    "\n",
    "    test_metrics = test_model(model, test_loader, device, threshold=0.5)\n",
    "    print(\"RISULTATI TEST FINALI:\")\n",
    "    print(f\"  Accuracy = {test_metrics['accuracy']:.3f}\")\n",
    "    print(f\"  Precision = {test_metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall = {test_metrics['recall']:.3f}\")\n",
    "    print(f\"  F1-score = {test_metrics['f1']:.3f}\")\n",
    "    print(f\"  IoU = {test_metrics['iou']:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c16131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = SINet(backbone_pretrained=True).to(device)\n",
    "# # model.load_state_dict(torch.load('sinet_pretrained_120epoch.pth'))\n",
    "\n",
    "# test_img_dir = 'COD10K-v3/Test/Image'\n",
    "# test_mask_dir = 'COD10K-v3/Test/GT_Object'\n",
    "\n",
    "# test_dataset = CODDataset(test_img_dir, test_mask_dir, transform=None)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=40, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# test_metrics, test_metrics2, test_metrics3 = test_model(model, test_loader, device, threshold=0.5)\n",
    "# print(\"RISULTATI TEST FINALI:\")\n",
    "# print(f\"  Accuracy = {test_metrics['accuracy']:.3f}\")\n",
    "# print(f\"  Precision = {test_metrics['precision']:.3f}\")\n",
    "# print(f\"  Recall = {test_metrics['recall']:.3f}\")\n",
    "# print(f\"  F1-score = {test_metrics['f1']:.3f}\")\n",
    "# print(f\"  IoU = {test_metrics['iou']:.3f}\")\n",
    "\n",
    "# print(f\"  Precision = {test_metrics2['Precision']:.3f}\")       # forse da eliminare se non utile alla valutazione\n",
    "# print(f\"  Recall = {test_metrics2['Recall']:.3f}\")\n",
    "# print(f\"  Dice = {test_metrics2['Dice']:.3f}\")\n",
    "# print(f\"  IoU = {test_metrics2['IoU']:.3f}\")\n",
    "\n",
    "# print(f\"  S-Measure = {test_metrics3['S-Measure']:.3f}\")       # forse da eliminare se non utile alla valutazione\n",
    "# print(f\"  E-Measure = {test_metrics3['E-Measure']:.3f}\")\n",
    "# print(f\"  WFM = {test_metrics3['WFM']:.3f}\")\n",
    "# print(f\"  MAE = {test_metrics3['MAE']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38548bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
